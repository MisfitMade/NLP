{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NLP_Lib import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import dtype\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from official.nlp import optimization # for creating custom optimizer\n",
    "\n",
    "from googletrans import constants # for translating to other langs\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# disable eager execution to use ELMo model\n",
    "# tf.disable_eager_execution()\n",
    "\n",
    "# improve/change plot appearance\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# load in the training data, make sure it is the form we expect\n",
    "training_df = pd.read_table(PATH_TO_TRAINING_TSV)\n",
    "assert (training_df.shape[1] > 1 and CLASS_COL == training_df.columns[1]\n",
    "    and training_df.shape[1] > 2 and SUBCLASS_COL == training_df.columns[2]\n",
    "    and training_df.shape[1] > 3 and TEXT_COL == training_df.columns[3])\n",
    "\n",
    "# shuffle\n",
    "training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "\n",
    "# remove any single word text column instances\n",
    "cleaned_training_df = training_df[training_df[TEXT_COL].str.split().str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Used the following method to grow our dataset\n",
    "increase_training_data_via_language_traslation(\n",
    "    cleaned_training_df,\n",
    "    0.001,\n",
    "    CLASSES,\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 104874 files belonging to 5 classes.\n",
      "Using 83900 files for training.\n",
      "Found 104874 files belonging to 5 classes.\n",
      "Using 20974 files for validation.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use the method \n",
    "\"make_df_into_ds_file_form\"\n",
    "in our Lib.py file to convert the training data .tsv into a folder/file structure like:\n",
    "./resources/training/\n",
    "___Bart/\n",
    "_______bart_instance_id_r.txt\n",
    "_______bart_instance_id_q.txt\n",
    "_______ ...\n",
    "___Homer/\n",
    "________homer_instance_id_k.txt\n",
    "________homer_instance_id_n.txt\n",
    "________ ...\n",
    "___...\n",
    "\n",
    "Then use the path to the root directory of the folder/file structure you just made, so\n",
    "for the above example, I let ./resources/training/ be \"PATH_TO_TRAINING_DATASET_STRUCT\"\n",
    "below and use\n",
    "tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size)\n",
    "to read the data set into a special form that is recognized by keras NN layers, as well\n",
    "as the instances are in the given \"batch_size\", which helps prevent overflowing RAM.\n",
    "'''\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b\"Don't worry. If I croak, you'll marry Lenny. Or Moe -- the winner will be determined by a card game I invented. I got all the rules written down... up here.\"\n",
      "Class : [0. 1. 0. 0. 0.] (Homer Simpson)\n",
      "\n",
      "Text: b'Now what have you done, Simpson?'\n",
      "Class : [0. 0. 0. 0. 1.] (Other)\n",
      "\n",
      "Text: b'Anything. You name it. What do you want to do?'\n",
      "Class : [0. 1. 0. 0. 0.] (Homer Simpson)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-16 09:56:31.267567: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Clean this data?\n",
    "\n",
    "# Can inspect the dataset structure made from the file folder struct\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Text: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Class : {label} ({class_names[np.nonzero(label)[0][0]]})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 65\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "init_lr = 3e-5\n",
    "optimizer_type = \"adamw\"\n",
    "optimizer = optimization.create_optimizer(\n",
    "    init_lr=init_lr,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    optimizer_type=optimizer_type)  # SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax Nadam, Ftrl\n",
    "\n",
    "model = build_classifier_model(SMALL_BERT[0], SMALL_BERT[1])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=tf.metrics.CategoricalAccuracy())\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "checkpoint_dir_x = os.path.join(\n",
    "    CHECKPOINT_DIR,\n",
    "    datetime.now().strftime(\"%m/%d/%H:%M:%S\"))\n",
    "pathlib.Path(checkpoint_dir_x).mkdir(parents=True, exist_ok=True)\n",
    "call_backs = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(checkpoint_dir_x, \"save_at_{epoch}.h5\"),\n",
    "        save_weights_only=True),\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=train_ds.with_options(options),\n",
    "    validation_data=val_ds.with_options(options),\n",
    "    verbose=1, # 0 is silent, 1 for loading bar, 2 for stats each epoch\n",
    "    epochs=epochs,\n",
    "    callbacks=call_backs)\n",
    "\n",
    "save_training_params(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    num_train_steps,\n",
    "    num_warmup_steps,\n",
    "    init_lr,\n",
    "    optimizer_type,\n",
    "    checkpoint_dir_x)\n",
    "plot_accuracy(plt, history, checkpoint_dir_x)\n",
    "plot_loss(plt, history, checkpoint_dir_x)\n",
    "confusion_matrix(plt, val_ds, model, checkpoint_dir_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb58a922583ee0cd0dc06a4d8a1ec3ba10d4c198521abe3b8d6a1e488418653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
