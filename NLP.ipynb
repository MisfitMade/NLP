{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NLP_Lib import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import dtype\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from googletrans import constants\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# disable eager execution to use ELMo model\n",
    "# tf.disable_eager_execution()\n",
    "\n",
    "# improve/change plot appearance\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# load in the training data, make sure it is the form we expect\n",
    "training_df = pd.read_table(PATH_TO_TRAINING_TSV)\n",
    "assert (training_df.shape[1] > 1 and CLASS_COL == training_df.columns[1]\n",
    "    and training_df.shape[1] > 2 and SUBCLASS_COL == training_df.columns[2]\n",
    "    and training_df.shape[1] > 3 and TEXT_COL == training_df.columns[3])\n",
    "\n",
    "# shuffle\n",
    "training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "\n",
    "# remove any single word text column instances\n",
    "cleaned_training_df = training_df[training_df[TEXT_COL].str.split().str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afrikaans\n",
      "albanian\n",
      "amharic\n",
      "arabic\n",
      "armenian\n",
      "azerbaijani\n",
      "basque\n",
      "belarusian\n",
      "bengali\n",
      "bosnian\n",
      "bulgarian\n",
      "catalan\n",
      "cebuano\n",
      "chichewa\n",
      "chinese (simplified)\n",
      "chinese (traditional)\n",
      "corsican\n",
      "croatian\n",
      "czech\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "esperanto\n",
      "estonian\n",
      "filipino\n",
      "finnish\n",
      "french\n",
      "frisian\n",
      "galician\n",
      "georgian\n",
      "german\n",
      "greek\n",
      "gujarati\n",
      "haitian creole\n",
      "hausa\n",
      "hawaiian\n",
      "hebrew\n",
      "hebrew\n",
      "hindi\n",
      "hmong\n",
      "hungarian\n",
      "icelandic\n",
      "igbo\n",
      "indonesian\n",
      "irish\n",
      "italian\n",
      "japanese\n",
      "javanese\n",
      "kannada\n",
      "kazakh\n",
      "khmer\n",
      "korean\n",
      "kurdish (kurmanji)\n",
      "kyrgyz\n",
      "lao\n",
      "latin\n",
      "latvian\n",
      "lithuanian\n",
      "luxembourgish\n",
      "macedonian\n",
      "malagasy\n",
      "malay\n",
      "malayalam\n",
      "maltese\n",
      "maori\n",
      "marathi\n",
      "mongolian\n",
      "myanmar (burmese)\n",
      "nepali\n",
      "norwegian\n",
      "odia\n",
      "pashto\n",
      "persian\n",
      "polish\n",
      "portuguese\n",
      "punjabi\n",
      "romanian\n",
      "russian\n",
      "samoan\n",
      "scots gaelic\n",
      "serbian\n",
      "sesotho\n",
      "shona\n",
      "sindhi\n",
      "sinhala\n",
      "slovak\n",
      "slovenian\n",
      "somali\n",
      "spanish\n",
      "sundanese\n",
      "swahili\n",
      "swedish\n",
      "tajik\n",
      "tamil\n",
      "telugu\n",
      "thai\n",
      "turkish\n",
      "ukrainian\n",
      "urdu\n",
      "uyghur\n",
      "uzbek\n",
      "vietnamese\n",
      "welsh\n",
      "xhosa\n",
      "yiddish\n",
      "yoruba\n",
      "zulu\n"
     ]
    }
   ],
   "source": [
    "for key in constants.LANGUAGES:\n",
    "    try:\n",
    "        # dont translate english to english then back to enlgish\n",
    "        if lang_i is not \"en\":\n",
    "            lang_i = constants.LANGUAGES[key]\n",
    "            print(lang_i)\n",
    "            translated_df = translate_df_to_and_back(key, cleaned_training_df, 0.01, CLASSES)\n",
    "            make_df_into_ds_file_form(translated_df, PATH_TO_TRAINING_DATASET_STRUCT, CLASSES, f\"{lang_i}_\")\n",
    "    except Exception:\n",
    "        print(\"Error on language: \", lang_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the method \n",
    "\"make_df_into_ds_file_form\"\n",
    "in our Lib.py file to convert the training data .tsv into a folder/file structure like:\n",
    "./resources/training/\n",
    "___Bart/\n",
    "_______bart_instance_id_r.txt\n",
    "_______bart_instance_id_q.txt\n",
    "_______ ...\n",
    "___Homer/\n",
    "________homer_instance_id_k.txt\n",
    "________homer_instance_id_n.txt\n",
    "________ ...\n",
    "___...\n",
    "\n",
    "Then use the path to the root directory of the folder/file structure you just made, so\n",
    "for the above example, I let ./resources/training/ be \"PATH_TO_TRAINING_DATASET_STRUCT\"\n",
    "below and use\n",
    "tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size)\n",
    "to read the data set into a special form that is recognized by keras NN layers, as well\n",
    "as the instances are in the given \"batch_size\", which helps prevent overflowing RAM.\n",
    "'''\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean this data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier_model(SMALL_BERT[0], SMALL_BERT[1])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), # SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax Nadam, Ftrl\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "checkpoint_dir_path_id = datetime.now().strftime(\"%m/%d/%H:%M:%S\")\n",
    "checkpoint_dir_x = os.path.join(CHECKPOINT_DIR, checkpoint_dir_path_id)\n",
    "pathlib.Path(checkpoint_dir_x).mkdir(parents=True, exist_ok=True)\n",
    "call_backs = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(checkpoint_dir_x, \"save_at_{epoch}.h5\"),\n",
    "        save_weights_only=True),\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "history = model.fit(\n",
    "    x=train_ds.with_options(options),\n",
    "    validation_data=val_ds.with_options(options),\n",
    "    verbose=1, # 0 is silent, 1 for loading bar, 2 for stats each epoch\n",
    "    epochs=epochs,\n",
    "    callbacks=call_backs)\n",
    "\n",
    "\n",
    "plot_accuracy(plt, history, checkpoint_dir_path_id)\n",
    "plot_loss(plt, history, checkpoint_dir_path_id)\n",
    "confusion_matrix(plt, val_ds, model, checkpoint_dir_path_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb58a922583ee0cd0dc06a4d8a1ec3ba10d4c198521abe3b8d6a1e488418653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
