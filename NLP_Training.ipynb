{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NLP_Lib import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from official.nlp import optimization # for creating custom optimizer\n",
    "\n",
    "from googletrans import constants # for translating to other langs\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# disable eager execution to use ELMo model\n",
    "# tf.disable_eager_execution()\n",
    "\n",
    "# improve/change plot appearance\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# load in the training data, make sure it is the form we expect\n",
    "training_df = pd.read_table(PATH_TO_TRAINING_TSV)\n",
    "assert (training_df.shape[1] > 1 and CLASS_COL == training_df.columns[1]\n",
    "    and training_df.shape[1] > 2 and SUBCLASS_COL == training_df.columns[2]\n",
    "    and training_df.shape[1] > 3 and TEXT_COL == training_df.columns[3])\n",
    "\n",
    "# shuffle\n",
    "training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "\n",
    "# remove any single word text column instances\n",
    "cleaned_training_df = training_df[training_df[TEXT_COL].str.split().str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Used the following method to grow our dataset\n",
    "increase_training_data_via_language_traslation(\n",
    "    cleaned_training_df,\n",
    "    0.001,\n",
    "    CLASSES,\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the method \n",
    "\"make_df_into_ds_file_form\"\n",
    "in our Lib.py file to convert the training data .tsv into a folder/file structure like:\n",
    "./resources/training/\n",
    "___Bart/\n",
    "_______bart_instance_id_r.txt\n",
    "_______bart_instance_id_q.txt\n",
    "_______ ...\n",
    "___Homer/\n",
    "________homer_instance_id_k.txt\n",
    "________homer_instance_id_n.txt\n",
    "________ ...\n",
    "___...\n",
    "\n",
    "Then use the path to the root directory of the folder/file structure you just made, so\n",
    "for the above example, I let ./resources/training/ be \"PATH_TO_TRAINING_DATASET_STRUCT\"\n",
    "below and use\n",
    "tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size)\n",
    "to read the data set into a special form that is recognized by keras NN layers, as well\n",
    "as the instances are in the given \"batch_size\", which helps prevent overflowing RAM.\n",
    "'''\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "validation_split=0.20\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=validation_split,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.shuffle(100).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=validation_split,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean this data?\n",
    "\n",
    "# Can use the following to inspect the dataset structure made from the file folder struct\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Text: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Class : {label} ({class_names[np.nonzero(label)[0][0]]})\\n')\n",
    "\n",
    "    \n",
    "texts, classes = [], []\n",
    "for x, label in val_ds:\n",
    "  texts.append(x)\n",
    "  classes.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with checkpoint saves, parmas, options\n",
    "epochs = 65\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.3*num_train_steps)\n",
    "init_lr = 5e-5\n",
    "optimizer_type = \"adamw\"\n",
    "optimizer = optimization.create_optimizer(\n",
    "    init_lr=init_lr,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    optimizer_type=optimizer_type)  # SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax Nadam, Ftrl\n",
    "\n",
    "model = build_classifier_model(SMALL_BERT[0], SMALL_BERT[1], optimizer)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a unique folder for this model and set up check points\n",
    "checkpoint_dir_x = os.path.join(\n",
    "    PATH_TO_MODELS_DIRECTORY,\n",
    "    datetime.now().strftime(\"%m/%d/%H:%M:%S\"))\n",
    "pathlib.Path(checkpoint_dir_x).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "call_backs = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(checkpoint_dir_x, \"save_at_{epoch}.h5\"),\n",
    "        save_weights_only=True),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5), # stop if the validation loss goes up for 5 epochs\n",
    "]\n",
    "\n",
    "# Train!\n",
    "history = model.fit(\n",
    "    x=train_ds.with_options(options),\n",
    "    validation_data=val_ds.with_options(options),\n",
    "    verbose=1, # 0 is silent, 1 for loading bar, 2 for stats each epoch\n",
    "    epochs=epochs,\n",
    "    callbacks=call_backs,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model summary, plus it's hyper params, epoch count, optimizer type, etc to a json to be\n",
    "# reloaded later.\n",
    "# Plot the mdoel's change in training and validation accuracy and loss over epochs.\n",
    "# Plot a confusion matrix using the validation data.\n",
    "print_model_summary_to_file(model, checkpoint_dir_x)\n",
    "save_training_params(\n",
    "    validation_split,\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    num_train_steps,\n",
    "    num_warmup_steps,\n",
    "    init_lr,\n",
    "    optimizer_type,\n",
    "    checkpoint_dir_x)\n",
    "plot_accuracy(plt, history, checkpoint_dir_x)\n",
    "plot_loss(plt, history, checkpoint_dir_x)\n",
    "confusion_matrix(plt, val_ds, model, checkpoint_dir_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb58a922583ee0cd0dc06a4d8a1ec3ba10d4c198521abe3b8d6a1e488418653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
