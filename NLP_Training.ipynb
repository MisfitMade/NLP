{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 12:25:54.460503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-18 12:25:54.460540: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NLP_Lib import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from official.nlp import optimization # for creating custom optimizer\n",
    "\n",
    "from googletrans import constants # for translating to other langs\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# disable eager execution to use ELMo model\n",
    "# tf.disable_eager_execution()\n",
    "\n",
    "# improve/change plot appearance\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# load in the training data, make sure it is the form we expect\n",
    "training_df = pd.read_table(PATH_TO_TRAINING_TSV)\n",
    "assert (training_df.shape[1] > 1 and CLASS_COL == training_df.columns[1]\n",
    "    and training_df.shape[1] > 2 and SUBCLASS_COL == training_df.columns[2]\n",
    "    and training_df.shape[1] > 3 and TEXT_COL == training_df.columns[3])\n",
    "\n",
    "# shuffle\n",
    "training_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "\n",
    "# remove any single word text column instances\n",
    "cleaned_training_df = training_df[training_df[TEXT_COL].str.split().str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Used the following method to grow our dataset\n",
    "increase_training_data_via_language_traslation(\n",
    "    cleaned_training_df,\n",
    "    0.001,\n",
    "    CLASSES,\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the method \n",
    "\"make_df_into_ds_file_form\"\n",
    "in our Lib.py file to convert the training data .tsv into a folder/file structure like:\n",
    "./resources/training/\n",
    "___Bart/\n",
    "_______bart_instance_id_r.txt\n",
    "_______bart_instance_id_q.txt\n",
    "_______ ...\n",
    "___Homer/\n",
    "________homer_instance_id_k.txt\n",
    "________homer_instance_id_n.txt\n",
    "________ ...\n",
    "___...\n",
    "\n",
    "Then use the path to the root directory of the folder/file structure you just made, so\n",
    "for the above example, I let ./resources/training/ be \"PATH_TO_TRAINING_DATASET_STRUCT\"\n",
    "below and use\n",
    "tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size)\n",
    "to read the data set into a special form that is recognized by keras NN layers, as well\n",
    "as the instances are in the given \"batch_size\", which helps prevent overflowing RAM.\n",
    "'''\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "validation_split=0.12\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=validation_split,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    PATH_TO_TRAINING_DATASET_STRUCT,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=validation_split,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b'Stupid shoes over tights! Why do I want to play with lame girls?'\n",
      "Class : [1. 0. 0. 0. 0.] (Bart Simpson)\n",
      "\n",
      "Text: b\"Sitting in a coffee shop, I couldn't feel more like a true writer! Oh better set up my wi-fi in case I need to do some digging! But if I'm going to use your free internet, I really should buy something.\"\n",
      "Class : [0. 0. 1. 0. 0.] (Lisa Simpson)\n",
      "\n",
      "Text: b'My first dollar! Thanks to you, Lisa, and our hemp smoking friend. You will be enlightened by diamonds that do not make you wise.'\n",
      "Class : [0. 0. 0. 0. 1.] (Other)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 12:28:27.916089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Clean this data?\n",
    "\n",
    "# Can inspect the dataset structure made from the file folder struct\n",
    "\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Text: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Class : {label} ({class_names[np.nonzero(label)[0][0]]})\\n')\n",
    "texts, classes = [], []\n",
    "for x, label in val_ds:\n",
    "  texts.append(x)\n",
    "  classes.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with checkpoint saves, parmas, options\n",
    "epochs = 65\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "init_lr = 3e-5\n",
    "optimizer_type = \"adamw\"\n",
    "optimizer = optimization.create_optimizer(\n",
    "    init_lr=init_lr,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    optimizer_type=optimizer_type)  # SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax Nadam, Ftrl\n",
    "\n",
    "model = build_classifier_model(SMALL_BERT[0], SMALL_BERT[1], optimizer)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "checkpoint_dir_x = os.path.join(\n",
    "    PATH_TO_MODELS_DIRECTORY,\n",
    "    datetime.now().strftime(\"%m/%d/%H:%M:%S\"))\n",
    "pathlib.Path(checkpoint_dir_x).mkdir(parents=True, exist_ok=True)\n",
    "call_backs = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(checkpoint_dir_x, \"save_at_{epoch}.h5\"),\n",
    "        save_weights_only=True),\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "history = model.fit(\n",
    "    x=train_ds.with_options(options),\n",
    "    validation_data=val_ds.with_options(options),\n",
    "    verbose=1, # 0 is silent, 1 for loading bar, 2 for stats each epoch\n",
    "    epochs=epochs,\n",
    "    callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's hyper params, epoch count, optimizer type, etc to a json to be\n",
    "# reloaded later.\n",
    "# Plot the mdoel's change in training and validation accuracy and loss over epochs.\n",
    "# Plot a confusion matrix using the validation data.\n",
    "save_training_params(\n",
    "    validation_split,\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    num_train_steps,\n",
    "    num_warmup_steps,\n",
    "    init_lr,\n",
    "    optimizer_type,\n",
    "    checkpoint_dir_x)\n",
    "plot_accuracy(plt, history, checkpoint_dir_x)\n",
    "plot_loss(plt, history, checkpoint_dir_x)\n",
    "confusion_matrix(plt, val_ds, model, checkpoint_dir_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb58a922583ee0cd0dc06a4d8a1ec3ba10d4c198521abe3b8d6a1e488418653c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
